#!/usr/bin/env python
"""
ç®€åŒ–ç‰ˆé‡‡é›†è„šæœ¬ï¼šåªé‡‡é›†ç±»ç›®å’Œå•†å“ä¿¡æ¯
"""
import os
import json
import asyncio
import logging
import re
import requests
from datetime import datetime
from urllib.parse import urljoin, urlparse
from pathlib import Path
from playwright.async_api import async_playwright

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("scraper_direct.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# åŸºç¡€é…ç½®
BASE_URL = "https://houseofsxn.com"
RESULTS_FILE = "scraped_categories_products.json"
TIMEOUT = 60000  # è¶…æ—¶è®¾ç½®(ms)
IMAGES_DIR = "scraped_images"  # å›¾ç‰‡ä¿å­˜ç›®å½•

# æµ‹è¯•æ¨¡å¼é…ç½®
TEST_MODE = os.environ.get('TEST_MODE', 'False').lower() in ('true', '1', 't')
MAX_PRODUCTS_PER_CATEGORY = 3 if TEST_MODE else None  # æ¯ä¸ªåˆ†ç±»æœ€å¤šæŠ“å–çš„å•†å“æ•°é‡

# æ— å¤´æ¨¡å¼é…ç½®
HEADLESS_MODE = os.environ.get('HEADLESS_MODE', 'True').lower() in ('true', '1', 't')

# è´§å¸æ¢ç®—æ¯”ç‡ (å‡è®¾ä¸ºé©¬æ¥è¥¿äºšæ—å‰ç‰¹å…‘ç¾å…ƒçš„æ¯”ç‡)
# æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´è¿™ä¸ªå€¼
MYR_TO_USD_RATE = 0.22  # 1 MYR â‰ˆ 0.22 USD

# ç¡®ä¿å›¾ç‰‡ä¿å­˜ç›®å½•å­˜åœ¨
Path(IMAGES_DIR).mkdir(exist_ok=True, parents=True)

async def scrape_categories(page):
    """æŠ“å–ç½‘ç«™çš„æ‰€æœ‰åˆ†ç±»"""
    logger.info("æ­£åœ¨æŠ“å–ç½‘ç«™åˆ†ç±»...")
    
    # ç¡®ä¿åŠ è½½ä¸»é¡µ
    try:
        await page.goto(BASE_URL, timeout=TIMEOUT)
        await page.wait_for_load_state('networkidle')
    except Exception as e:
        logger.error(f"åŠ è½½é¡µé¢å¤±è´¥: {e}")
    
    # è·å–é¡¶çº§åˆ†ç±»
    categories = []
    
    # å°è¯•ä»ä¸»å¯¼èˆªä¸­è·å–åˆ†ç±»
    nav_elements = await page.query_selector_all('.site-nav__item a')
    for nav_element in nav_elements:
        try:
            name = await nav_element.text_content()
            url = await nav_element.get_attribute('href')
            if url:
                full_url = urljoin(BASE_URL, url)
                
                # æ’é™¤å•†å“é“¾æ¥å’Œå¸¦æœ‰ä»·æ ¼æ ‡è®°çš„é“¾æ¥
                if 'products/' in full_url or '$' in name:
                    continue
                    
                # ä»…ä¿ç•™collectionç±»å‹çš„é“¾æ¥
                if '/collections/' in full_url:
                    categories.append({
                        'name': name.strip(),
                        'url': full_url,
                        'parent': None
                    })
                    logger.info(f"æ‰¾åˆ°é¡¶çº§åˆ†ç±»: {name} - {full_url}")
        except Exception as e:
            logger.error(f"æŠ“å–åˆ†ç±»é¡¹ç›®æ—¶å‡ºé”™: {e}")
    
    logger.info(f"æ‰¾åˆ° {len(categories)} ä¸ªé¡¶çº§åˆ†ç±»é¡¹ç›®")
    
    # å¦‚æœä¸»å¯¼èˆªæ²¡æœ‰åˆ†ç±»ï¼Œå°è¯•å…¶ä»–æ–¹å¼è·å–
    if len(categories) == 0:
        logger.info("ä¸»å¯¼èˆªæœªæ‰¾åˆ°åˆ†ç±»ï¼Œå°è¯•å…¶ä»–æ–¹å¼è·å–åˆ†ç±»")
        
        # è·å–é¡¶éƒ¨èœå•ä¸­çš„"SHOP"é“¾æ¥
        shop_link = None
        shop_elements = await page.query_selector_all('a')
        for element in shop_elements:
            try:
                text = await element.text_content()
                if 'SHOP' in text or 'Shop' in text or 'Collection' in text or 'Collections' in text:
                    url = await element.get_attribute('href')
                    if url and '/collections/' in url:
                        shop_link = urljoin(BASE_URL, url)
                        break
            except:
                continue
        
        # å¦‚æœæ‰¾åˆ°SHOPé“¾æ¥ï¼Œè·å–å…¶ä¸­çš„åˆ†ç±»
        if shop_link:
            logger.info(f"æ‰¾åˆ°SHOPé“¾æ¥: {shop_link}")
            await page.goto(shop_link)
            await page.wait_for_load_state('networkidle')
            
            # è·å–åˆ†ç±»é“¾æ¥
            collection_links = await page.query_selector_all('a')
            for link in collection_links:
                try:
                    href = await link.get_attribute('href')
                    if href and '/collections/' in href and 'products/' not in href:
                        name = await link.text_content()
                        name = name.strip()
                        
                        # æ’é™¤å•†å“é“¾æ¥å’Œå¸¦æœ‰ä»·æ ¼æ ‡è®°çš„é“¾æ¥
                        if '$' in name or len(name) > 50:
                            continue
                            
                        url = urljoin(BASE_URL, href)
                        
                        # æ£€æŸ¥æ˜¯å¦å·²æ·»åŠ ç›¸åŒçš„åˆ†ç±»
                        duplicate = False
                        for cat in categories:
                            if cat['name'] == name or cat['url'] == url:
                                duplicate = True
                                break
                                
                        if not duplicate:
                            categories.append({
                                'name': name,
                                'url': url,
                                'parent': None
                            })
                            logger.info(f"æ‰¾åˆ°å¤‡ç”¨åˆ†ç±»: {name} - {url}")
                except Exception as e:
                    # å¿½ç•¥é”™è¯¯ç»§ç»­
                    pass
    
    logger.info(f"å…±æ‰¾åˆ° {len(categories)} ä¸ªåˆ†ç±»")
    return categories

async def scrape_product_details(page, product_url, category_name, parent_category_name=''):
    """æŠ“å–å•ä¸ªå•†å“è¯¦æƒ…"""
    logger.info(f"æ­£åœ¨æŠ“å–å•†å“è¯¦æƒ…: {product_url}")
    
    try:
        await page.goto(product_url, timeout=TIMEOUT)
        await page.wait_for_load_state('networkidle')
        
        # ä»schema.orgç»“æ„åŒ–æ•°æ®ä¸­æå–å•†å“ä¿¡æ¯
        schema_data = await page.evaluate("""
            () => {
                const scripts = Array.from(document.querySelectorAll('script[type="application/ld+json"]'));
                for (const script of scripts) {
                    try {
                        const data = JSON.parse(script.textContent);
                        if (data['@type'] === 'Product') {
                            return data;
                        }
                    } catch (e) {
                        console.error('Error parsing JSON-LD:', e);
                    }
                }
                return null;
            }
        """)
        
        if not schema_data:
            logger.warning(f"å•†å“ {product_url} æœªæ‰¾åˆ°schema.orgæ•°æ®")
            return None
        
        # æå–å•†å“æ ‡é¢˜
        title = schema_data.get('name', 'æœªçŸ¥å•†å“')
        
        # ä»é¡µé¢å†…å®¹è·å–å•†å“æè¿°ï¼Œè€Œéschema.orgä¸­çš„æè¿°
        description = await page.evaluate("""
            () => {
                // å°è¯•æŸ¥æ‰¾äº§å“æè¿°å®¹å™¨
                const productDescriptionSelectors = [
                    '.product-single__description',
                    '.product__description',
                    '.product-description',
                    '#product-description',
                    '[id*="product-description"]',
                    '[class*="product-description"]',
                    '.rte',  // ä¸€äº›Shopifyä¸»é¢˜ä½¿ç”¨è¿™ä¸ªç±»
                    '.tabs-content [id*="description"]',
                    '.tabs-content [id*="Description"]'
                ];
                
                for (const selector of productDescriptionSelectors) {
                    const el = document.querySelector(selector);
                    if (el && el.textContent.trim()) {
                        // æ¸…ç†æè¿°ï¼Œç§»é™¤å¤šä½™ç©ºç™½
                        let text = el.innerHTML;
                        return text;
                    }
                }
                
                // å¤‡ç”¨æ–¹æ¡ˆï¼šå°è¯•æŸ¥æ‰¾æœ€å¯èƒ½åŒ…å«äº§å“æè¿°çš„å…ƒç´ 
                const possibleContainers = [
                    'main',
                    '#MainContent',
                    '.main-content',
                    '.product-template',
                    '.product-single'
                ];
                
                for (const container of possibleContainers) {
                    const el = document.querySelector(container);
                    if (el) {
                        // åœ¨å®¹å™¨ä¸­æŸ¥æ‰¾æè¿°æ€§æ®µè½
                        const paragraphs = el.querySelectorAll('p, .rte p, [id*="description"] p');
                        if (paragraphs.length > 0) {
                            // ç»„åˆæ‰€æœ‰æ®µè½ï¼Œå¿½ç•¥å¾ˆçŸ­çš„ï¼ˆå¯èƒ½æ˜¯ä»·æ ¼ç­‰ï¼‰
                            let combinedText = "";
                            for (const p of paragraphs) {
                                const text = p.textContent.trim();
                                if (text.length > 20) {  // å¿½ç•¥å¤ªçŸ­çš„æ®µè½
                                    combinedText += "<p>" + p.innerHTML + "</p>";
                                }
                            }
                            if (combinedText) {
                                return combinedText;
                            }
                        }
                    }
                }
                
                // å¦‚æœä¸Šè¿°æ–¹æ³•éƒ½å¤±è´¥ï¼Œè¿”å›ä¸€ä¸ªç©ºå­—ç¬¦ä¸²
                return "";
            }
        """)
        
        # å¦‚æœä»é¡µé¢æ— æ³•è·å–æè¿°ï¼Œå›é€€åˆ°schema.orgæ•°æ®ä¸­çš„æè¿°
        if not description:
            description = schema_data.get('description', '')
            logger.info(f"ä»schema.orgè·å–å•†å“æè¿°: {description[:100]}...")
        else:
            logger.info(f"ä»é¡µé¢å†…å®¹è·å–å•†å“æè¿° (é•¿åº¦: {len(description)})")
        
        # æå–å•†å“SKU
        sku = schema_data.get('sku', '')
        
        # æå–å•†å“å“ç‰Œ
        brand = schema_data.get('brand', '')
        
        # æå–å•†å“ä»·æ ¼ (ä»offersä¸­è·å–)
        price_myr = 0
        currency = 'USD'
        if 'offers' in schema_data and isinstance(schema_data['offers'], list) and len(schema_data['offers']) > 0:
            offer = schema_data['offers'][0]
            price_myr = float(offer.get('price', 0))
            currency = offer.get('priceCurrency', 'USD')
        
        # å°†ä»·æ ¼è½¬æ¢ä¸ºç¾å…ƒ
        price_usd = round(price_myr * MYR_TO_USD_RATE, 2) if currency == 'MYR' else price_myr
        
        # æå–å•†å“å›¾ç‰‡
        images = []
        
        # è·å–å½“å‰å•†å“URLä¸­çš„å…³é”®éƒ¨åˆ†ï¼Œç”¨äºå›¾ç‰‡åŒ¹é…
        product_key = product_url.split('/')[-1].split('?')[0]
        logger.info(f"å½“å‰å•†å“å…³é”®è¯: {product_key}")
        
        # æ›´é«˜æ•ˆçš„å›¾ç‰‡è·å–é€»è¾‘
        try:
            # 1. é¦–å…ˆå°è¯•ä»äº§å“å›¾åº“ä¸­æå–å½“å‰å•†å“å›¾ç‰‡
            js_images = await page.evaluate("""
                (productKey) => {
                    // è·å–å½“å‰å•†å“çš„å›¾ç‰‡
                    const images = [];
                    
                    // æŸ¥æ‰¾ä¸»å›¾ç‰‡å’Œå˜ä½“å›¾ç‰‡
                    const productPhotoContainer = document.querySelector('.product__photos');
                    if (productPhotoContainer) {
                        // è·å–å¯è§çš„æ‰€æœ‰å¤§å›¾URL (ä¸»å›¾åŒºåŸŸ)
                        const mainImgs = productPhotoContainer.querySelectorAll('img.photoswipe__image');
                        mainImgs.forEach(img => {
                            const src = img.src || img.dataset.src || '';
                            if (src) {
                                let highResSrc = src;
                                if (src.includes('width=')) {
                                    highResSrc = src.replace(/width=\\d+/, 'width=2000');
                                }
                                images.push(highResSrc);
                            }
                        });
                    }
                    
                    // å¦‚æœæ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡ï¼Œå°è¯•è·å–ç¼©ç•¥å›¾
                    if (images.length === 0) {
                        const thumbContainer = document.querySelector('[id^="ProductThumbs-"]');
                        if (thumbContainer) {
                            const thumbs = thumbContainer.querySelectorAll('li img');
                            thumbs.forEach(img => {
                                const src = img.src || img.dataset.src || '';
                                if (src) {
                                    let highResSrc = src;
                                    if (src.includes('width=')) {
                                        highResSrc = src.replace(/width=\\d+/, 'width=2000');
                                    }
                                    if (src.includes('_compact')) {
                                        highResSrc = src.replace('_compact', '');
                                    }
                                    images.push(highResSrc);
                                }
                            });
                        }
                    }
                    
                    // å¦‚æœä»ç„¶æ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡ï¼Œä»é¡µé¢ä¸Šæ‰€æœ‰å›¾ç‰‡ä¸­è¿‡æ»¤
                    if (images.length === 0) {
                        const allImages = Array.from(document.querySelectorAll('img'));
                        // æ ¹æ®URLä¸­åŒ…å«å•†å“å…³é”®è¯æˆ–altæ–‡æœ¬ç›¸å…³æ¥ç­›é€‰
                        const productImages = allImages.filter(img => {
                            const src = img.src || img.dataset.src || '';
                            const alt = img.alt || '';
                            
                            const matchesProductKey = 
                                productKey && 
                                ((src && src.toLowerCase().includes(productKey.toLowerCase())) || 
                                (alt && alt.toLowerCase().includes(productKey.toLowerCase())));
                                
                            const isProductImage = 
                                (src && src.includes('/products/')) && 
                                !src.includes('related-product') && 
                                !src.includes('recommendation');
                                
                            return matchesProductKey || isProductImage;
                        });
                        
                        productImages.forEach(img => {
                            const src = img.src || img.dataset.src || '';
                            if (src) {
                                let highResSrc = src;
                                if (src.includes('width=')) {
                                    highResSrc = src.replace(/width=\\d+/, 'width=2000');
                                }
                                images.push(highResSrc);
                            }
                        });
                    }
                    
                    return [...new Set(images)]; // å»é‡
                }
            """, product_key)
            
            if js_images and isinstance(js_images, list):
                for img_url in js_images:
                    if isinstance(img_url, str):
                        if img_url.startswith('//'):
                            img_url = 'https:' + img_url
                        elif not img_url.startswith(('http://', 'https://')):
                            img_url = urljoin(BASE_URL, img_url)
                        
                        if img_url not in images:
                            images.append(img_url)
                            logger.info(f"ä»äº§å“å›¾åº“æ‰¾åˆ°å›¾ç‰‡: {img_url}")
            
            # 2. å¦‚æœä¸Šè¿°æ–¹æ³•æ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡ï¼Œå°è¯•ä½¿ç”¨ProductThumbsé€‰æ‹©å™¨
            if not images:
                logger.info("å°è¯•ä½¿ç”¨ProductThumbs-IDé€‰æ‹©å™¨æå–å›¾ç‰‡")
                thumbs_id = await page.evaluate("""
                    () => {
                        const thumbsEl = document.querySelector('[id^="ProductThumbs-"]');
                        return thumbsEl ? thumbsEl.id : null;
                    }
                """)
                
                if thumbs_id:
                    logger.info(f"æ‰¾åˆ°ç¼©ç•¥å›¾å®¹å™¨: {thumbs_id}")
                    thumb_elements = await page.query_selector_all(f"#{thumbs_id} li")
                    logger.info(f"æ‰¾åˆ° {len(thumb_elements)} ä¸ªç¼©ç•¥å›¾å…ƒç´ ")
                    
                    for thumb in thumb_elements:
                        img = await thumb.query_selector('img')
                        if img:
                            # è·å–ç¼©ç•¥å›¾çš„æ•°æ®æºå±æ€§
                            data_src = await img.get_attribute('data-src') or await img.get_attribute('src')
                            if data_src:
                                # è½¬æ¢ç¼©ç•¥å›¾URLä¸ºé«˜åˆ†è¾¨ç‡å›¾ç‰‡URL
                                high_res_url = data_src.replace('_compact', '')
                                high_res_url = high_res_url.replace('width=160', 'width=2000')
                                
                                if high_res_url.startswith('//'):
                                    high_res_url = 'https:' + high_res_url
                                elif not high_res_url.startswith(('http://', 'https://')):
                                    high_res_url = urljoin(BASE_URL, high_res_url)
                                
                                if high_res_url not in images:
                                    images.append(high_res_url)
                                    logger.info(f"ä»ProductThumbsæ‰¾åˆ°å›¾ç‰‡: {high_res_url}")
        
        except Exception as e:
            logger.warning(f"æå–å•†å“å›¾ç‰‡å¤±è´¥: {e}")
        
        # 3. å¦‚æœä»æœªæ‰¾åˆ°å›¾ç‰‡ï¼Œä»JSON-LDç»“æ„åŒ–æ•°æ®ä¸­æå–
        if not images and 'image' in schema_data:
            image_data = schema_data['image']
            if isinstance(image_data, list):
                for img in image_data:
                    if isinstance(img, str):
                        images.append(img)
                    elif isinstance(img, dict) and 'url' in img:
                        images.append(img['url'])
            elif isinstance(image_data, str):
                images.append(image_data)
            elif isinstance(image_data, dict) and 'url' in image_data:
                images.append(image_data['url'])
        
        # å»é™¤é‡å¤çš„å›¾ç‰‡URLå¹¶è§„èŒƒåŒ–
        clean_images = []
        local_images = []  # ä¿å­˜æœ¬åœ°å›¾ç‰‡è·¯å¾„
        
        # ä¸ºå½“å‰å•†å“åˆ›å»ºä¸“å±æ–‡ä»¶å¤¹
        product_folder_name = re.sub(r'[^\w\-]', '_', product_key)  # å®‰å…¨çš„æ–‡ä»¶å¤¹å
        product_image_dir = os.path.join(IMAGES_DIR, product_folder_name)
        
        # ç¡®ä¿å•†å“å›¾ç‰‡ç›®å½•å­˜åœ¨
        Path(product_image_dir).mkdir(exist_ok=True, parents=True)
        logger.info(f"åˆ›å»ºå•†å“å›¾ç‰‡ç›®å½•: {product_image_dir}")
        
        for img_url in images:
            if img_url.startswith('//'):
                img_url = 'https:' + img_url
            elif not img_url.startswith(('http://', 'https://')):
                img_url = urljoin(BASE_URL, img_url)
            
            # ä¼˜åŒ–å›¾ç‰‡URL (å»é™¤å®½åº¦å’Œä¸å¿…è¦çš„å‚æ•°)
            parsed_url = urlparse(img_url)
            path = parsed_url.path
            # åªä¿ç•™vå‚æ•°ï¼Œå»é™¤widthç­‰å…¶ä»–å‚æ•°
            if parsed_url.query:
                params = parsed_url.query.split('&')
                v_param = next((p for p in params if p.startswith('v=')), None)
                if v_param:
                    img_url = f"https://{parsed_url.netloc}{path}?{v_param}"
                else:
                    img_url = f"https://{parsed_url.netloc}{path}"
            
            # ä¿å­˜å›¾ç‰‡åˆ°æœ¬åœ°
            if img_url not in clean_images:
                clean_images.append(img_url)
                
                # ç”Ÿæˆæœ¬åœ°æ–‡ä»¶å
                filename = os.path.basename(path)
                local_path = os.path.join(product_folder_name, filename)  # ä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼Œä¸å«IMAGES_DIR
                full_path = os.path.join(product_image_dir, filename)     # å®é™…ä¿å­˜æ—¶ä½¿ç”¨çš„å®Œæ•´è·¯å¾„
                
                try:
                    # ä¸‹è½½å›¾ç‰‡
                    response = requests.get(img_url, stream=True, timeout=30)
                    if response.status_code == 200:
                        with open(full_path, 'wb') as f:
                            for chunk in response.iter_content(chunk_size=8192):
                                f.write(chunk)
                        local_images.append(local_path)  # ä¿å­˜ç›¸å¯¹è·¯å¾„ï¼Œç”¨äºæ•°æ®åº“è®°å½•
                        logger.info(f"å›¾ç‰‡å·²ä¿å­˜åˆ°æœ¬åœ°: {full_path}")
                    else:
                        logger.warning(f"ä¸‹è½½å›¾ç‰‡å¤±è´¥: {img_url}, çŠ¶æ€ç : {response.status_code}")
                except Exception as e:
                    logger.warning(f"ä¸‹è½½å›¾ç‰‡å¼‚å¸¸: {img_url}, é”™è¯¯: {e}")
        
        images = clean_images
        logger.info(f"å•†å“å…±æ‰¾åˆ° {len(images)} å¼ å›¾ç‰‡, æˆåŠŸä¸‹è½½ {len(local_images)} å¼ åˆ°æœ¬åœ°ç›®å½•: {product_image_dir}")
        
        # è·å–å•†å“åº“å­˜çŠ¶æ€
        in_stock = False
        if 'offers' in schema_data and isinstance(schema_data['offers'], list) and len(schema_data['offers']) > 0:
            offer = schema_data['offers'][0]
            availability = offer.get('availability', '')
            in_stock = 'InStock' in availability
        
        # åˆ›å»ºå•†å“å¯¹è±¡
        product = {
            'title': title,
            'url': product_url,
            'price_original': price_myr,  # åŸå§‹ä»·æ ¼
            'currency_original': currency,  # åŸå§‹è´§å¸
            'price_usd': price_usd,  # è½¬æ¢ä¸ºç¾å…ƒçš„ä»·æ ¼
            'description': description,
            'sku': sku,
            'brand': brand,
            'in_stock': in_stock,
            'main_image': images[0] if images else '',
            'images': images,
            'local_images': local_images,  # æ·»åŠ æœ¬åœ°å›¾ç‰‡è·¯å¾„
            'category': category_name,
            'parent_category': parent_category_name,
            'scraped_at': datetime.now().isoformat()
        }
        
        logger.info(f"æˆåŠŸæŠ“å–å•†å“è¯¦æƒ…: {title}")
        return product
    
    except Exception as e:
        logger.error(f"æŠ“å–å•†å“è¯¦æƒ…å¤±è´¥: {product_url}, é”™è¯¯: {e}")
        return None

async def scrape_products_in_category(page, category):
    """æŠ“å–å•ä¸ªåˆ†ç±»ä¸­çš„æ‰€æœ‰å•†å“"""
    category_name = category['name']
    category_url = category['url']
    parent_category_name = category.get('parent', '')
    logger.info(f"æ­£åœ¨æŠ“å–åˆ†ç±» '{category_name}' ä¸­çš„å•†å“: {category_url}")
    
    products = []
    product_urls = []
    
    try:
        await page.goto(category_url, timeout=TIMEOUT)
        await page.wait_for_load_state('networkidle')
        
        # ä¿®å¤ï¼šä½¿ç”¨JavaScriptæå–æ‰€æœ‰å•†å“é“¾æ¥ï¼Œè¿™æ ·æ›´å¯é 
        product_links = await page.evaluate("""
            () => {
                const links = [];
                
                // å°è¯•å„ç§å¯èƒ½çš„é€‰æ‹©å™¨ç»„åˆ
                // å…ˆæŸ¥æ‰¾æ‰€æœ‰å•†å“é¡¹
                document.querySelectorAll('.grid__item, .grid-product, .product-item, .product-card, .product').forEach(item => {
                    // åœ¨æ¯ä¸ªå•†å“é¡¹ä¸­æŸ¥æ‰¾é“¾æ¥
                    const link = item.querySelector('a[href*="/products/"]');
                    if (link) {
                        const href = link.getAttribute('href');
                        // ç¡®ä¿é“¾æ¥æ˜¯å®Œæ•´URL
                        const fullUrl = href.startsWith('/') ? window.location.origin + href : href;
                        links.push(fullUrl);
                    }
                });
                
                // å¦‚æœä¸Šé¢çš„æ–¹æ³•æ²¡æ‰¾åˆ°é“¾æ¥ï¼Œå°è¯•ç›´æ¥æŸ¥æ‰¾æ‰€æœ‰å•†å“é“¾æ¥
                if (links.length === 0) {
                    document.querySelectorAll('a.grid-product__link, a.grid-view-item__link, a.product-card__link, a.product-link, a[href*="/products/"]').forEach(link => {
                        const href = link.getAttribute('href');
                        if (href && href.includes('/products/')) {
                            const fullUrl = href.startsWith('/') ? window.location.origin + href : href;
                            links.push(fullUrl);
                        }
                    });
                }
                
                return [...new Set(links)]; // å»é‡
            }
        """)
        
        logger.info(f"åœ¨åˆ†ç±» '{category_name}' ä¸­æ‰¾åˆ° {len(product_links)} ä¸ªå•†å“é“¾æ¥")
        print(f"ğŸ“Š åˆ†ç±» '{category_name}' ä¸­æ‰¾åˆ° {len(product_links)} ä¸ªå•†å“é“¾æ¥")
        
        # åœ¨æµ‹è¯•æ¨¡å¼ä¸‹é™åˆ¶å•†å“æ•°é‡
        if TEST_MODE and MAX_PRODUCTS_PER_CATEGORY and len(product_links) > MAX_PRODUCTS_PER_CATEGORY:
            product_links = product_links[:MAX_PRODUCTS_PER_CATEGORY]
            logger.info(f"æµ‹è¯•æ¨¡å¼: é™åˆ¶ä¸ºå‰ {len(product_links)} ä¸ªå•†å“")
        
        # æŠ“å–æ¯ä¸ªå•†å“è¯¦æƒ…
        for i, url in enumerate(product_links, 1):
            logger.info(f"æ­£åœ¨æŠ“å–å•†å“ {i}/{len(product_links)}: {url}")
            print(f"â³ æ­£åœ¨æŠ“å–å•†å“ {i}/{len(product_links)}: {url}")
            
            product_info = await scrape_product_details(page, url, category_name, parent_category_name)
            if product_info:
                products.append(product_info)
                logger.info(f"æˆåŠŸæŠ“å–å•†å“: {product_info['title']}")
                print(f"  âœ“ æˆåŠŸ: {product_info['title']}")
            else:
                logger.warning(f"æŠ“å–å¤±è´¥: {url}")
                print(f"  âœ— å¤±è´¥: {url}")
            
            # æ·»åŠ çŸ­æš‚å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
            await asyncio.sleep(1)
        
        return products
    
    except Exception as e:
        logger.error(f"æŠ“å–åˆ†ç±» '{category_name}' ä¸­çš„å•†å“å¤±è´¥: {e}")
        return []

async def main():
    """ä¸»å‡½æ•°: æŠ“å–æ‰€æœ‰åˆ†ç±»åŠå…¶å•†å“"""
    all_categories = []
    all_products = []
    start_time = datetime.now()
    
    logger.info("=== å¼€å§‹é‡‡é›†ä»»åŠ¡ ===")
    logger.info(f"é‡‡é›†ç›®æ ‡ç½‘ç«™: {BASE_URL}")
    if TEST_MODE:
        logger.info("è¿è¡Œåœ¨æµ‹è¯•æ¨¡å¼ï¼Œå°†ä»…é‡‡é›†æœ‰é™çš„å•†å“")
    logger.info(f"æµè§ˆå™¨æ¨¡å¼: {'æ— å¤´æ¨¡å¼' if HEADLESS_MODE else 'å¯è§†æ¨¡å¼'}")
    
    # åˆ›å»ºè¿›åº¦è®¡æ•°å™¨
    products_counter = 0
    images_counter = 0
    
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=HEADLESS_MODE)
        context = await browser.new_context(
            viewport={"width": 1280, "height": 800},
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        )
        page = await context.new_page()
        
        # æŠ“å–æ‰€æœ‰åˆ†ç±»
        categories = await scrape_categories(page)
        all_categories = categories
        logger.info(f"å…±æ‰¾åˆ° {len(categories)} ä¸ªåˆ†ç±»")
        print(f"ğŸ“Š å…±æ‰¾åˆ° {len(categories)} ä¸ªåˆ†ç±»")
        
        # æŠ“å–æ¯ä¸ªåˆ†ç±»ä¸­çš„å•†å“
        for i, category in enumerate(categories, 1):
            logger.info(f"=== å¼€å§‹é‡‡é›†ç¬¬ {i}/{len(categories)} ä¸ªåˆ†ç±»: {category['name']} ===")
            print(f"â³ æ­£åœ¨é‡‡é›†åˆ†ç±» ({i}/{len(categories)}): {category['name']}")
            
            category_products = await scrape_products_in_category(page, category)
            all_products.extend(category_products)
            
            # æ›´æ–°è®¡æ•°å™¨
            products_counter += len(category_products)
            images_counter += sum(len(product.get('images', [])) for product in category_products)
            
            logger.info(f"=== å®Œæˆç¬¬ {i}/{len(categories)} ä¸ªåˆ†ç±»é‡‡é›†ï¼Œè·å– {len(category_products)} ä¸ªå•†å“ ===")
            print(f"âœ… å®Œæˆåˆ†ç±» {category['name']}: é‡‡é›† {len(category_products)} ä¸ªå•†å“, ç´¯è®¡ {products_counter} ä¸ªå•†å“, {images_counter} å¼ å›¾ç‰‡")
            
            # æ·»åŠ å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
            await asyncio.sleep(2)
        
        await browser.close()
    
    # åˆå¹¶åˆ†ç±»å’Œå•†å“æ•°æ®
    result = {
        'categories': all_categories,
        'products': all_products,
        'total_categories': len(all_categories),
        'total_products': len(all_products),
        'scraped_at': datetime.now().isoformat()
    }
    
    # ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶
    with open(RESULTS_FILE, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()
    
    logger.info("=== é‡‡é›†ä»»åŠ¡å®Œæˆ! ===")
    logger.info(f"å…±é‡‡é›† {len(all_categories)} ä¸ªåˆ†ç±», {len(all_products)} ä¸ªå•†å“, {images_counter} å¼ å›¾ç‰‡")
    logger.info(f"è€—æ—¶: {duration:.2f} ç§’ (å¹³å‡æ¯ä¸ªå•†å“ {duration/max(1, len(all_products)):.2f} ç§’)")
    logger.info(f"é‡‡é›†ç»“æœå·²ä¿å­˜åˆ° {RESULTS_FILE}")
    
    print(f"\nğŸ‰ é‡‡é›†ä»»åŠ¡å®Œæˆ!")
    print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"  - å…±é‡‡é›† {len(all_categories)} ä¸ªåˆ†ç±»")
    print(f"  - å…±é‡‡é›† {len(all_products)} ä¸ªå•†å“")
    print(f"  - å…±é‡‡é›† {images_counter} å¼ å›¾ç‰‡")
    print(f"  - è€—æ—¶: {duration//60} åˆ† {duration%60:.0f} ç§’")
    print(f"  - é‡‡é›†ç»“æœå·²ä¿å­˜åˆ° {RESULTS_FILE}")
    
    return True

if __name__ == "__main__":
    asyncio.run(main()) 