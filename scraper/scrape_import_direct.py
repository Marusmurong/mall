#!/usr/bin/env python
"""
ä¸€é”®å¼é‡‡é›†å¯¼å…¥è„šæœ¬ï¼šé‡‡é›†å•†å“å¹¶ç›´æ¥å¯¼å…¥æ•°æ®åº“
"""
import os
import sys
import json
import asyncio
import logging
import django
import traceback
from datetime import datetime
from pathlib import Path
from asgiref.sync import sync_to_async

# è®¾ç½®Djangoç¯å¢ƒ
BASE_DIR = Path(__file__).resolve().parent.parent
sys.path.append(str(BASE_DIR))
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'mall.settings')
django.setup()

# å¯¼å…¥Djangoæ¨¡å‹
from goods.models import GoodsCategory, Goods, GoodsImage
from scraper.scrape_direct import (
    scrape_categories, 
    scrape_products_in_category,
    HEADLESS_MODE
)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("scrape_import_direct.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# åŸºç¡€é…ç½®
BASE_URL = "https://houseofsxn.com"
IMAGES_DIR = "scraped_images"  # å›¾ç‰‡ä¿å­˜ç›®å½•

# æµ‹è¯•æ¨¡å¼é…ç½®
TEST_MODE = os.environ.get('TEST_MODE', 'False').lower() in ('true', '1', 't')
MAX_CATEGORIES = 2 if TEST_MODE else None  # æµ‹è¯•æ¨¡å¼ä¸‹æœ€å¤šå¤„ç†çš„åˆ†ç±»æ•°é‡

# ç¡®ä¿å›¾ç‰‡ä¿å­˜ç›®å½•å­˜åœ¨
Path(IMAGES_DIR).mkdir(exist_ok=True, parents=True)

# ç»Ÿè®¡è®¡æ•°å™¨
stats = {
    'categories_found': 0,
    'categories_imported': 0,
    'products_found': 0,
    'products_imported': 0,
    'products_updated': 0,
    'products_skipped': 0,
    'images_found': 0,
    'images_imported': 0
}

# å®šä¹‰åŒæ­¥ç‰ˆæœ¬çš„å‡½æ•°
def import_category_sync(category_data, category_map=None):
    """å¯¼å…¥å•ä¸ªåˆ†ç±»ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰"""
    if category_map is None:
        category_map = {}
    
    category_name = category_data['name']
    parent_name = category_data.get('parent', None)
    
    try:
        # æ£€æŸ¥åˆ†ç±»æ˜¯å¦å·²ç»å­˜åœ¨
        category, created = GoodsCategory.objects.get_or_create(
            name=category_name,
            defaults={
                'description': f'ä»{BASE_URL}é‡‡é›†çš„åˆ†ç±»',
                'level': 1,
                'is_active': True,
                'sort_order': 0
            }
        )
        
        # è®°å½•åˆ†ç±»IDæ˜ å°„
        category_map[category_name] = category.id
        
        if created:
            logger.info(f"æ–°å»ºåˆ†ç±»: {category_name} (ID: {category.id})")
        else:
            logger.info(f"åˆ†ç±»å·²å­˜åœ¨: {category_name} (ID: {category.id})")
        
        # è¿”å›åˆ†ç±»å¯¹è±¡å’Œæ˜ å°„
        return category, category_map, created
    
    except Exception as e:
        logger.error(f"å¯¼å…¥åˆ†ç±» '{category_name}' å¤±è´¥: {e}")
        traceback.print_exc()
        return None, category_map, False

# ä½¿ç”¨sync_to_asyncåŒ…è£…åŒæ­¥å‡½æ•°
import_category = sync_to_async(import_category_sync)

def set_parent_categories_sync(category_map, categories):
    """è®¾ç½®åˆ†ç±»çš„çˆ¶å­å…³ç³»ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰"""
    for category_data in categories:
        category_name = category_data['name']
        parent_name = category_data.get('parent', None)
        
        if parent_name and parent_name in category_map and category_name in category_map:
            try:
                category = GoodsCategory.objects.get(id=category_map[category_name])
                parent_category = GoodsCategory.objects.get(id=category_map[parent_name])
                
                # è®¾ç½®çˆ¶åˆ†ç±»
                category.parent_category = parent_category
                category.save()
                
                logger.info(f"è®¾ç½®åˆ†ç±» '{category_name}' çš„çˆ¶åˆ†ç±»ä¸º '{parent_name}'")
            
            except Exception as e:
                logger.error(f"è®¾ç½®åˆ†ç±» '{category_name}' çš„çˆ¶åˆ†ç±»å…³ç³»å¤±è´¥: {e}")

# ä½¿ç”¨sync_to_asyncåŒ…è£…åŒæ­¥å‡½æ•°
set_parent_categories = sync_to_async(set_parent_categories_sync)

def import_product_sync(product_data, category_map):
    """å¯¼å…¥å•ä¸ªå•†å“ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰"""
    title = product_data.get('title', '')
    category_name = product_data.get('category', '')
    parent_category_name = product_data.get('parent_category', '')
    
    # å¦‚æœæ²¡æœ‰æ ‡é¢˜ï¼Œè·³è¿‡å¯¼å…¥
    if not title:
        logger.warning(f"è·³è¿‡å¯¼å…¥å•†å“: æ ‡é¢˜ä¸ºç©º")
        return None, False, False, 0
    
    # è·å–åˆ†ç±»ID
    category_id = None
    if category_name in category_map:
        category_id = category_map[category_name]
    elif parent_category_name in category_map:
        # å¦‚æœæ‰¾ä¸åˆ°ç›´æ¥åˆ†ç±»ï¼Œå°è¯•ä½¿ç”¨çˆ¶åˆ†ç±»
        category_id = category_map[parent_category_name]
        logger.info(f"æœªæ‰¾åˆ°åˆ†ç±» '{category_name}'ï¼Œä½¿ç”¨çˆ¶åˆ†ç±» '{parent_category_name}'")
    
    if not category_id:
        logger.warning(f"è·³è¿‡å¯¼å…¥å•†å“ '{title}': æœªæ‰¾åˆ°å¯¹åº”çš„åˆ†ç±»ID")
        return None, False, False, 0
    
    # è·å–ä»·æ ¼ä¿¡æ¯
    price = 0
    if 'price_usd' in product_data and product_data['price_usd']:
        price = float(product_data['price_usd'])
    elif 'price_original' in product_data and product_data['price_original']:
        price = float(product_data['price_original'])
    
    # è·å–å•†å“æè¿°
    description = product_data.get('description', '')
    
    # è·å–å•†å“URL
    source_url = product_data.get('url', '')
    
    # è·å–å•†å“çŠ¶æ€
    is_on_sale = product_data.get('in_stock', True)
    
    # è·å–å•†å“å›¾ç‰‡
    images = product_data.get('images', [])
    local_images = product_data.get('local_images', [])
    main_image = product_data.get('main_image', '')
    
    try:
        # æ£€æŸ¥å•†å“æ˜¯å¦å·²å­˜åœ¨
        try:
            # å…ˆé€šè¿‡source_urlæŸ¥æ‰¾
            goods = Goods.objects.get(source_url=source_url)
            created = False
            logger.info(f"é€šè¿‡source_urlæ‰¾åˆ°å·²å­˜åœ¨å•†å“: {title}")
        except Goods.DoesNotExist:
            # å†é€šè¿‡åç§°å’Œåˆ†ç±»æŸ¥æ‰¾
            try:
                goods = Goods.objects.get(name=title, category_id=category_id)
                created = False
                logger.info(f"é€šè¿‡åç§°å’Œåˆ†ç±»æ‰¾åˆ°å·²å­˜åœ¨å•†å“: {title}")
            except Goods.DoesNotExist:
                # åˆ›å»ºæ–°å•†å“
                goods = Goods()
                created = True
        
        # æ›´æ–°æˆ–åˆ›å»ºå•†å“ä¿¡æ¯
        goods.name = title
        goods.category_id = category_id
        goods.description = description
        goods.goods_desc = description  # ä½¿ç”¨ç›¸åŒçš„æè¿°ä¿¡æ¯
        goods.price = price
        goods.original_price = price  # å¯ä»¥è®¾ç½®åŸä»·ç­‰äºå”®ä»·ï¼Œæˆ–è€…ç¨é«˜ä¸€äº›
        goods.source_url = source_url
        goods.status = 'published' if is_on_sale else 'off_shelf'
        goods.is_recommended = True  # æ–°å¯¼å…¥çš„å•†å“è®¾ä¸ºæ¨è
        goods.is_hot = is_on_sale
        goods.is_new = True
        goods.stock = 100  # æ‰€æœ‰å•†å“ç»Ÿä¸€è®¾ç½®åº“å­˜ä¸º100
        
        # ä¿å­˜å•†å“
        goods.save()
        
        # å¤„ç†å•†å“å›¾ç‰‡
        images_imported = 0
        if local_images:
            # ä¼˜å…ˆä½¿ç”¨æœ¬åœ°å›¾ç‰‡ï¼Œç¡®ä¿å·²ä¸‹è½½åˆ°æœ¬åœ°
            for i, img_path in enumerate(local_images):
                if os.path.exists(img_path):
                    # åˆ›å»ºå›¾ç‰‡å¯¹è±¡
                    image, img_created = GoodsImage.objects.get_or_create(
                        goods=goods,
                        image=img_path.replace('scraped_images/', ''),
                        defaults={
                            'is_main': i == 0,  # ç¬¬ä¸€å¼ å›¾è®¾ä¸ºä¸»å›¾
                            'sort_order': i
                        }
                    )
                    
                    if img_created:
                        logger.info(f"ä¸ºå•†å“ '{title}' æ·»åŠ å›¾ç‰‡: {img_path}")
                        images_imported += 1
        elif images:
            # å¦‚æœæ²¡æœ‰æœ¬åœ°å›¾ç‰‡ï¼Œä½¿ç”¨è¿œç¨‹å›¾ç‰‡URL
            for i, img_url in enumerate(images):
                # æå–å›¾ç‰‡æ–‡ä»¶åï¼Œå»æ‰åŸŸåå’ŒæŸ¥è¯¢å‚æ•°
                img_filename = img_url.split('/')[-1].split('?')[0]
                img_category_dir = title.lower().replace(' ', '-')
                
                # æ„å»ºæ­£ç¡®çš„ç›¸å¯¹è·¯å¾„ - ä¸è¦åŒ…å«å®Œæ•´URL
                relative_path = f"{img_category_dir}/{img_filename}"
                
                # åˆ›å»ºå›¾ç‰‡å¯¹è±¡ - ä½¿ç”¨ç›¸å¯¹è·¯å¾„æ ¼å¼
                image, img_created = GoodsImage.objects.get_or_create(
                    goods=goods,
                    image=relative_path,
                    defaults={
                        'is_main': i == 0,  # ç¬¬ä¸€å¼ å›¾è®¾ä¸ºä¸»å›¾
                        'sort_order': i
                    }
                )
                
                if img_created:
                    logger.info(f"ä¸ºå•†å“ '{title}' æ·»åŠ è¿œç¨‹å›¾ç‰‡: {img_url} -> {relative_path}")
                    images_imported += 1
        
        # è®¾ç½®ä¸»å›¾
        if not goods.image and images:
            img_url = images[0]
            img_filename = img_url.split('/')[-1].split('?')[0]
            img_category_dir = title.lower().replace(' ', '-')
            
            # åŒæ ·ä½¿ç”¨ç›¸å¯¹è·¯å¾„ä½œä¸ºä¸»å›¾
            goods.image = f"{img_category_dir}/{img_filename}"
            goods.save(update_fields=['image'])
        
        if created:
            logger.info(f"æ–°å»ºå•†å“: {title} (ID: {goods.id})")
        else:
            logger.info(f"æ›´æ–°å•†å“: {title} (ID: {goods.id})")
        
        return goods, created, True, images_imported
    
    except Exception as e:
        logger.error(f"å¯¼å…¥å•†å“ '{title}' å¤±è´¥: {e}")
        traceback.print_exc()
        return None, False, False, 0

# ä½¿ç”¨sync_to_asyncåŒ…è£…åŒæ­¥å‡½æ•°
import_product = sync_to_async(import_product_sync)

async def main():
    """ä¸»å‡½æ•°: é‡‡é›†å¹¶å¯¼å…¥æ‰€æœ‰åˆ†ç±»å’Œå•†å“"""
    start_time = datetime.now()
    
    logger.info("=== å¼€å§‹é‡‡é›†å¹¶å¯¼å…¥ä»»åŠ¡ ===")
    logger.info(f"é‡‡é›†ç›®æ ‡ç½‘ç«™: {BASE_URL}")
    if TEST_MODE:
        logger.info("è¿è¡Œåœ¨æµ‹è¯•æ¨¡å¼ï¼Œå°†ä»…å¤„ç†æœ‰é™çš„åˆ†ç±»å’Œå•†å“")
    logger.info(f"æµè§ˆå™¨æ¨¡å¼: {'æ— å¤´æ¨¡å¼' if HEADLESS_MODE else 'å¯è§†æ¨¡å¼'}")
    
    print("\nğŸš€ å¼€å§‹é‡‡é›†å¹¶å¯¼å…¥æ•°æ®...")
    print(f"ğŸ” é‡‡é›†ç›®æ ‡: {BASE_URL}")
    if TEST_MODE:
        print("âš ï¸ æµ‹è¯•æ¨¡å¼: ä»…å¤„ç†æœ‰é™çš„æ•°æ®")
    
    from playwright.async_api import async_playwright
    
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=HEADLESS_MODE)
        context = await browser.new_context(
            viewport={"width": 1280, "height": 800},
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        )
        page = await context.new_page()
        
        # é‡‡é›†æ‰€æœ‰åˆ†ç±»
        categories = await scrape_categories(page)
        stats['categories_found'] = len(categories)
        logger.info(f"å…±æ‰¾åˆ° {len(categories)} ä¸ªåˆ†ç±»")
        print(f"ğŸ“Š å…±æ‰¾åˆ° {len(categories)} ä¸ªåˆ†ç±»")
        
        # åœ¨æµ‹è¯•æ¨¡å¼ä¸‹é™åˆ¶åˆ†ç±»æ•°é‡
        if TEST_MODE and MAX_CATEGORIES and len(categories) > MAX_CATEGORIES:
            categories = categories[:MAX_CATEGORIES]
            logger.info(f"æµ‹è¯•æ¨¡å¼: é™åˆ¶ä¸ºå‰ {len(categories)} ä¸ªåˆ†ç±»")
            print(f"âš ï¸ æµ‹è¯•æ¨¡å¼: ä»…å¤„ç†å‰ {len(categories)} ä¸ªåˆ†ç±»")
        
        # å¯¼å…¥åˆ†ç±»
        category_map = {}  # ç”¨äºå­˜å‚¨åˆ†ç±»åç§°åˆ°IDçš„æ˜ å°„
        
        print("\nğŸ“ ç¬¬ä¸€æ­¥: å¯¼å…¥åˆ†ç±»...")
        for i, category in enumerate(categories, 1):
            print(f"  å¤„ç†åˆ†ç±» ({i}/{len(categories)}): {category['name']}")
            category_obj, category_map, created = await import_category(category, category_map)
            
            if created:
                stats['categories_imported'] += 1
        
        # è®¾ç½®åˆ†ç±»çš„çˆ¶å­å…³ç³»
        await set_parent_categories(category_map, categories)
        
        # é‡‡é›†å¹¶å¯¼å…¥æ¯ä¸ªåˆ†ç±»ä¸­çš„å•†å“
        print("\nğŸ›ï¸ ç¬¬äºŒæ­¥: é‡‡é›†å¹¶å¯¼å…¥å•†å“...")
        for i, category in enumerate(categories, 1):
            category_name = category['name']
            print(f"\nâ³ æ­£åœ¨å¤„ç†åˆ†ç±» ({i}/{len(categories)}): {category_name}")
            
            # é‡‡é›†åˆ†ç±»ä¸­çš„å•†å“
            category_products = await scrape_products_in_category(page, category)
            stats['products_found'] += len(category_products)
            stats['images_found'] += sum(len(product.get('images', [])) for product in category_products)
            
            logger.info(f"åˆ†ç±» '{category_name}' ä¸­æ‰¾åˆ° {len(category_products)} ä¸ªå•†å“")
            print(f"ğŸ“Š åˆ†ç±» '{category_name}' ä¸­æ‰¾åˆ° {len(category_products)} ä¸ªå•†å“")
            
            # å¯¼å…¥å•†å“
            products_imported = 0
            products_updated = 0
            products_skipped = 0
            images_imported = 0
            for j, product in enumerate(category_products, 1):
                product_title = product.get('title', f'æœªçŸ¥å•†å“ {j}')
                print(f"  å¤„ç†å•†å“ ({j}/{len(category_products)}): {product_title}")
                
                # å¯¼å…¥å•†å“
                goods_obj, created, success, img_count = await import_product(product, category_map)
                
                if success:
                    if created:
                        products_imported += 1
                        stats['products_imported'] += 1
                    else:
                        products_updated += 1
                        stats['products_updated'] += 1
                    
                    # æ›´æ–°å¯¼å…¥çš„å›¾ç‰‡æ•°
                    images_imported += img_count
                    stats['images_imported'] += img_count
                else:
                    products_skipped += 1
                    stats['products_skipped'] += 1
            
            logger.info(f"åˆ†ç±» '{category_name}' å¯¼å…¥ç»“æœ: æ–°å¢ {products_imported} ä¸ª, æ›´æ–° {products_updated} ä¸ª, è·³è¿‡ {products_skipped} ä¸ª")
            print(f"âœ… åˆ†ç±» '{category_name}' å¯¼å…¥ç»“æœ: æ–°å¢ {products_imported} ä¸ª, æ›´æ–° {products_updated} ä¸ª, è·³è¿‡ {products_skipped} ä¸ª")
        
        await browser.close()
    
    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()
    
    logger.info("=== é‡‡é›†å¹¶å¯¼å…¥ä»»åŠ¡å®Œæˆ! ===")
    logger.info(f"ç»Ÿè®¡ä¿¡æ¯:")
    logger.info(f"- åˆ†ç±»: å‘ç° {stats['categories_found']} ä¸ª, å¯¼å…¥ {stats['categories_imported']} ä¸ª")
    logger.info(f"- å•†å“: å‘ç° {stats['products_found']} ä¸ª, å¯¼å…¥ {stats['products_imported']} ä¸ª, æ›´æ–° {stats['products_updated']} ä¸ª, è·³è¿‡ {stats['products_skipped']} ä¸ª")
    logger.info(f"- å›¾ç‰‡: å‘ç° {stats['images_found']} å¼ , å¯¼å…¥ {stats['images_imported']} å¼ ")
    logger.info(f"- è€—æ—¶: {duration:.2f} ç§’")
    
    print(f"\nğŸ‰ é‡‡é›†å¹¶å¯¼å…¥ä»»åŠ¡å®Œæˆ!")
    print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"  - åˆ†ç±»: å‘ç° {stats['categories_found']} ä¸ª, å¯¼å…¥ {stats['categories_imported']} ä¸ª")
    print(f"  - å•†å“: å‘ç° {stats['products_found']} ä¸ª, å¯¼å…¥ {stats['products_imported']} ä¸ª, æ›´æ–° {stats['products_updated']} ä¸ª, è·³è¿‡ {stats['products_skipped']} ä¸ª")
    print(f"  - å›¾ç‰‡: å‘ç° {stats['images_found']} å¼ , å¯¼å…¥ {stats['images_imported']} å¼ ")
    print(f"  - è€—æ—¶: {duration//60:.0f} åˆ† {duration%60:.0f} ç§’")
    
    return True

if __name__ == "__main__":
    asyncio.run(main()) 