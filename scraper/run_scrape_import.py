#!/usr/bin/env python
"""
ä¸€é”®è¿è¡Œè„šæœ¬ï¼šæ‰§è¡Œé‡‡é›†å’Œå¯¼å…¥è¿‡ç¨‹
"""
import os
import sys
import json
import time
import logging
import asyncio
from pathlib import Path

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("scrape_import.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ°Pythonè·¯å¾„
BASE_DIR = Path(__file__).resolve().parent.parent
sys.path.append(str(BASE_DIR))

def print_header(text):
    """æ‰“å°ç¾è§‚çš„æ ‡é¢˜å¤´"""
    print("\n" + "=" * 80)
    print(f" {text} ".center(80, "="))
    print("=" * 80 + "\n")

def ask_yes_no(question):
    """è¯¢é—®ç”¨æˆ·æ˜¯/å¦é—®é¢˜"""
    while True:
        response = input(f"{question} (y/n): ").lower().strip()
        if response in ['y', 'yes']:
            return True
        elif response in ['n', 'no']:
            return False
        print("è¯·è¾“å…¥ 'y' æˆ– 'n'.")

async def run_scraper(test_mode=False, headless=True):
    """è¿è¡Œé‡‡é›†è„šæœ¬"""
    print_header("å¼€å§‹é‡‡é›†å•†å“æ•°æ®")
    
    # è®¾ç½®æµ‹è¯•æ¨¡å¼ç¯å¢ƒå˜é‡
    if test_mode:
        os.environ['TEST_MODE'] = 'True'
        logger.info("å¯ç”¨æµ‹è¯•æ¨¡å¼ - æ¯ä¸ªåˆ†ç±»å°†åªé‡‡é›†å°‘é‡å•†å“")
    else:
        os.environ['TEST_MODE'] = 'False'
        logger.info("å¯ç”¨å®Œæ•´é‡‡é›†æ¨¡å¼ - å°†é‡‡é›†æ‰€æœ‰åˆ†ç±»å’Œå•†å“")
    
    # è®¾ç½®æ— å¤´æ¨¡å¼ç¯å¢ƒå˜é‡
    os.environ['HEADLESS_MODE'] = 'True' if headless else 'False'
    logger.info(f"æµè§ˆå™¨é‡‡é›†æ¨¡å¼: {'æ— å¤´æ¨¡å¼' if headless else 'å¯è§†æ¨¡å¼'}")
    
    try:
        # å¯¼å…¥é‡‡é›†è„šæœ¬
        from scraper.scrape_direct import main as scraper_main
        
        # è¿è¡Œé‡‡é›†
        logger.info("å¼€å§‹è¿è¡Œé‡‡é›†è„šæœ¬...")
        await scraper_main()
        
        return True
    except Exception as e:
        logger.error(f"é‡‡é›†è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        return False

def run_import():
    """è¿è¡Œå¯¼å…¥è„šæœ¬"""
    print_header("å¼€å§‹å¯¼å…¥æ•°æ®åˆ°æ•°æ®åº“")
    
    try:
        # è®¾ç½®Djangoç¯å¢ƒ
        os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'mall.settings')
        import django
        django.setup()
        
        # å¯¼å…¥å¯¼å…¥è„šæœ¬
        from scraper.import_categories_products import import_data
        
        # æ£€æŸ¥é‡‡é›†çš„æ•°æ®æ–‡ä»¶
        from scraper.scrape_direct import RESULTS_FILE
        
        if not os.path.exists(RESULTS_FILE):
            logger.error(f"é‡‡é›†æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {RESULTS_FILE}")
            return False
        
        # åŠ è½½é‡‡é›†çš„æ•°æ®
        with open(RESULTS_FILE, 'r', encoding='utf-8') as f:
            scraped_data = json.load(f)
        
        # è¿è¡Œå¯¼å…¥
        logger.info("å¼€å§‹å¯¼å…¥æ•°æ®...")
        import_data(scraped_data)
        
        return True
    except Exception as e:
        logger.error(f"å¯¼å…¥è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        return False

async def main():
    """ä¸»å‡½æ•°ï¼šäº¤äº’å¼è¿è¡Œé‡‡é›†å’Œå¯¼å…¥è¿‡ç¨‹"""
    print_header("å•†å“é‡‡é›†ä¸å¯¼å…¥ç³»ç»Ÿ")
    print("è¿™ä¸ªè„šæœ¬å°†å¸®åŠ©ä½ å®Œæˆä»ç½‘ç«™é‡‡é›†å•†å“æ•°æ®å¹¶å¯¼å…¥åˆ°æ•°æ®åº“çš„è¿‡ç¨‹ã€‚\n")
    
    # è¯¢é—®æµ‹è¯•æ¨¡å¼
    test_mode = ask_yes_no("æ˜¯å¦ä½¿ç”¨æµ‹è¯•æ¨¡å¼è¿è¡Œ? (æµ‹è¯•æ¨¡å¼ä¸‹åªé‡‡é›†å°‘é‡å•†å“)")
    
    # è¯¢é—®æ— å¤´æ¨¡å¼
    headless = ask_yes_no("æ˜¯å¦ä½¿ç”¨æ— å¤´æ¨¡å¼è¿è¡Œ? (æ— å¤´æ¨¡å¼ä¸‹ä¸ä¼šæ˜¾ç¤ºæµè§ˆå™¨çª—å£)")
    
    # è¿è¡Œé‡‡é›†
    scrape_success = await run_scraper(test_mode, headless)
    if not scrape_success:
        print("\nâŒ é‡‡é›†è¿‡ç¨‹å‡ºé”™ï¼Œè¯·æ£€æŸ¥æ—¥å¿—æ–‡ä»¶è·å–è¯¦ç»†ä¿¡æ¯ã€‚")
        return
    
    print("\nâœ… é‡‡é›†å®Œæˆ!")
    
    # æ˜¾ç¤ºé‡‡é›†ç»“æœç»Ÿè®¡
    try:
        # ä»æ–‡ä»¶åŠ è½½é‡‡é›†çš„æ•°æ®
        from scraper.scrape_direct import RESULTS_FILE
        
        if os.path.exists(RESULTS_FILE):
            with open(RESULTS_FILE, 'r', encoding='utf-8') as f:
                scraped_data = json.load(f)
                
            categories_count = len(scraped_data.get('categories', []))
            products_count = len(scraped_data.get('products', []))
            
            print("\nğŸ“Š é‡‡é›†ç»Ÿè®¡:")
            print(f"  - æ€»è®¡é‡‡é›†äº† {categories_count} ä¸ªå•†å“ç±»ç›®")
            print(f"  - æ€»è®¡é‡‡é›†äº† {products_count} ä¸ªå•†å“")
            
            # ç»Ÿè®¡æ¯ä¸ªç±»ç›®çš„å•†å“æ•°é‡
            category_stats = {}
            for product in scraped_data.get('products', []):
                category = product.get('category', 'æœªåˆ†ç±»')
                category_stats[category] = category_stats.get(category, 0) + 1
            
            print("\nğŸ“Š å„ç±»ç›®å•†å“æ•°é‡:")
            for category, count in sorted(category_stats.items(), key=lambda x: x[1], reverse=True):
                print(f"  - {category}: {count} ä¸ªå•†å“")
                
            # ç»Ÿè®¡å›¾ç‰‡æ•°é‡
            total_images = sum(len(product.get('images', [])) for product in scraped_data.get('products', []))
            print(f"\nğŸ“Š æ€»è®¡é‡‡é›†äº† {total_images} å¼ å•†å“å›¾ç‰‡")
    except Exception as e:
        logger.error(f"ç»Ÿè®¡é‡‡é›†ç»“æœæ—¶å‡ºé”™: {e}")
    
    # è¯¢é—®æ˜¯å¦ç»§ç»­å¯¼å…¥
    if ask_yes_no("\næ˜¯å¦å°†é‡‡é›†çš„æ•°æ®å¯¼å…¥åˆ°æ•°æ®åº“?"):
        import_success = run_import()
        if import_success:
            print("\nâœ… å¯¼å…¥å®Œæˆ! æ•°æ®å·²æˆåŠŸå¯¼å…¥åˆ°æ•°æ®åº“ã€‚")
        else:
            print("\nâŒ å¯¼å…¥è¿‡ç¨‹å‡ºé”™ï¼Œè¯·æ£€æŸ¥æ—¥å¿—æ–‡ä»¶è·å–è¯¦ç»†ä¿¡æ¯ã€‚")
    else:
        print("\nâ¹ï¸ å·²è·³è¿‡å¯¼å…¥æ­¥éª¤ã€‚é‡‡é›†çš„æ•°æ®å·²ä¿å­˜åˆ°JSONæ–‡ä»¶ã€‚")
    
    print("\næ„Ÿè°¢ä½¿ç”¨å•†å“é‡‡é›†ä¸å¯¼å…¥ç³»ç»Ÿ!")

if __name__ == "__main__":
    asyncio.run(main()) 